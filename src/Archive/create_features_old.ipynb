{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used to create feature sets\n",
    "N-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import functions \n",
    "classif = functions.classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8703\n",
      "HUDOC-ECHR-1999-001-58225\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Open cases\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_alpha'), 'r') as readfile:\n",
    "    text = json.load(readfile)\n",
    "    readfile.close()\n",
    "    \n",
    "print(len(text.keys()))\n",
    "print(list(text.keys())[0])\n",
    "print(len(text['HUDOC-ECHR-2012-001-110881']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8703\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>37</th>\n",
       "      <th>41</th>\n",
       "      <th>46</th>\n",
       "      <th>P1</th>\n",
       "      <th>P4</th>\n",
       "      <th>P12</th>\n",
       "      <th>P7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUDOC-ECHR-1999-001-58225</td>\n",
       "      <td>1999-03-25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUDOC-ECHR-1999-001-58226</td>\n",
       "      <td>1999-03-25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUDOC-ECHR-1999-001-58227</td>\n",
       "      <td>1999-03-25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUDOC-ECHR-1999-001-58239</td>\n",
       "      <td>1999-04-29</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUDOC-ECHR-1999-001-58251</td>\n",
       "      <td>1999-05-20</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id        date  2  3  5  6  7  8  9  10  ...  19  \\\n",
       "0  HUDOC-ECHR-1999-001-58225  1999-03-25 -1 -1  0 -1 -1 -1 -1  -1  ...  -1   \n",
       "1  HUDOC-ECHR-1999-001-58226  1999-03-25 -1 -1 -1  0 -1 -1 -1  -1  ...  -1   \n",
       "2  HUDOC-ECHR-1999-001-58227  1999-03-25 -1 -1 -1  2 -1  2 -1  -1  ...  -1   \n",
       "3  HUDOC-ECHR-1999-001-58239  1999-04-29 -1 -1  0 -1 -1 -1 -1  -1  ...  -1   \n",
       "4  HUDOC-ECHR-1999-001-58251  1999-05-20  0 -1 -1 -1 -1 -1 -1  -1  ...  -1   \n",
       "\n",
       "   34  35  37  41  46  P1  P4  P12  P7  \n",
       "0  -1  -1  -1   2  -1  -1  -1   -1  -1  \n",
       "1  -1  -1  -1   2  -1  -1  -1   -1  -1  \n",
       "2  -1   2  -1   2  -1   2  -1   -1  -1  \n",
       "3  -1   2  -1   2  -1  -1  -1   -1  -1  \n",
       "4  -1   2  -1   2  -1  -1  -1   -1  -1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check target \n",
    "target = pd.read_csv('/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/target.csv')\n",
    "print(len(target))\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_stopwords =  pickle.load( open( \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/stopwords.pickle\", \"rb\" ) )\n",
    "len(legal_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSet(article,part,max_features,ngram_range,stop_words):\n",
    "    \"\"\"\n",
    "    returns the feature set corrisponding to article and part \n",
    "    \"\"\"\n",
    "    df = target[['id',article]]\n",
    "    \n",
    "    #Get balanced dataset\n",
    "    nvLen = len(df[df[article] == 0]) \n",
    "    vLen = len(df[df[article] == 1])\n",
    "    minLen = min(nvLen,vLen)\n",
    "\n",
    "    nvID = df[df[article] == 0][0:minLen]['id']\n",
    "    vID = df[df[article] == 1][0:minLen]['id']\n",
    "    \n",
    "    nvCorpus = []\n",
    "    vCorpus = []\n",
    "    for ID in nvID:\n",
    "        nvCorpus.append(text[ID][part])\n",
    "\n",
    "    for ID in vID:\n",
    "        vCorpus.append(text[ID][part])\n",
    "        \n",
    "    corpus = nvCorpus + vCorpus\n",
    "    targets = ['nonviolation']*minLen + ['violation']*minLen\n",
    "    \n",
    "    #Vectorise\n",
    "    #\n",
    "    vectorizer = CountVectorizer(max_features= max_features,ngram_range=ngram_range,stop_words =stop_words)\n",
    "    X = vectorizer.fit_transform(corpus,y=targets)\n",
    "    X.toarray()\n",
    "    \n",
    "    #Features\n",
    "    features = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    features['target'] = targets\n",
    "    \n",
    "    return features\n",
    "    \n",
    "article = '6'\n",
    "part = 'facts'\n",
    "max_features = 2000\n",
    "ngram_range = (1,4)\n",
    "\n",
    "\n",
    "#features = featureSet(article,part,max_features,ngram_range)\n",
    "\n",
    "#features.to_csv()\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/Documents/Legal-Case-Prediction/data/features/english/article{}_{}.csv\n",
      "3\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "6\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "8\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n"
     ]
    }
   ],
   "source": [
    "file ='english'\n",
    "stop_words = 'english'\n",
    "\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/features/{}/article{}_{}.csv\".format(file,'{}','{}')\n",
    "\n",
    "print(path)\n",
    "articles = ['3','6','8']\n",
    "parts = [\"full\",\"procedure\",\"facts\",\"circumstances\",\"relevant\",\"law\",\"other\"]\n",
    "max_features = 2000\n",
    "ngram_range = (1,4)\n",
    "\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    for part in parts:\n",
    "        print(part)\n",
    "        features = featureSet(article,part,max_features,ngram_range,stop_words)\n",
    "        features.to_csv(path.format(article,part),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal-stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/Documents/Legal-Case-Prediction/data/features/legal/article{}_{}.csv\n",
      "3\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "6\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "8\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n"
     ]
    }
   ],
   "source": [
    "file ='legal'\n",
    "stop_words = legal_stopwords\n",
    "\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/features/{}/article{}_{}.csv\".format(file,'{}','{}')\n",
    "\n",
    "print(path)\n",
    "articles = ['3','6','8']\n",
    "parts = [\"full\",\"procedure\",\"facts\",\"circumstances\",\"relevant\",\"law\",\"other\"]\n",
    "max_features = 2000\n",
    "ngram_range = (1,4)\n",
    "\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    for part in parts:\n",
    "        print(part)\n",
    "        features = featureSet(article,part,max_features,ngram_range,stop_words)\n",
    "        features.to_csv(path.format(article,part),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### None stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/Documents/Legal-Case-Prediction/data/features/none/article{}_{}.csv\n",
      "3\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "6\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n",
      "8\n",
      "full\n",
      "procedure\n",
      "facts\n",
      "circumstances\n",
      "relevant\n",
      "law\n",
      "other\n"
     ]
    }
   ],
   "source": [
    "file ='none'\n",
    "stop_words = None\n",
    "\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/features/{}/article{}_{}.csv\".format(file,'{}','{}')\n",
    "\n",
    "print(path)\n",
    "articles = ['3','6','8']\n",
    "parts = [\"full\",\"procedure\",\"facts\",\"circumstances\",\"relevant\",\"law\",\"other\"]\n",
    "max_features = 2000\n",
    "ngram_range = (1,4)\n",
    "\n",
    "for article in articles:\n",
    "    print(article)\n",
    "    for part in parts:\n",
    "        print(part)\n",
    "        features = featureSet(article,part,max_features,ngram_range,stop_words)\n",
    "        features.to_csv(path.format(article,part),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
