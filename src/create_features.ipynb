{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import functions \n",
    "clean = functions.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/train_test_split_update.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b6f0088ff681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_test_split_update'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreadfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_test_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreadfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/train_test_split_update.json'"
     ]
    }
   ],
   "source": [
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('train_test_split_update'), 'r') as readfile:\n",
    "    train_test_split = json.load(readfile)\n",
    "    readfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/text_none.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-489e01b87c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreadfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtext_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreadfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/text_none.json'"
     ]
    }
   ],
   "source": [
    "#Open cases\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_none'), 'r') as readfile:\n",
    "    text_none = json.load(readfile)\n",
    "    readfile.close()\n",
    "    \n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_english'), 'r') as readfile:\n",
    "    text_english = json.load(readfile)\n",
    "    readfile.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HUDOC-ECHR-1982-001-57580'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_english.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 595 1008 504 595 539\n"
     ]
    }
   ],
   "source": [
    "# Fucntions\n",
    "def getPart(ID,part,stopwords):\n",
    "    \"\"\"\n",
    "    Returns clean case part\n",
    "    \"\"\"\n",
    "    \n",
    "    if stopwords == 'none':\n",
    "        doc = text_none[ID][part]\n",
    "    elif stopwords == 'english':\n",
    "        doc = text_english[ID][part]\n",
    "    return doc\n",
    "\n",
    "\n",
    "def getCorpus(article,part,stopwords):\n",
    "    \"\"\"\n",
    "    returns the train test corpus along with target\n",
    "    \"\"\"\n",
    "    split = train_test_split[article]\n",
    "    \n",
    "    train_corpus = []\n",
    "    test_corpus = []\n",
    "    \n",
    "    for ID in split['v_train'] + split['nv_train']:\n",
    "        doc = getPart(ID,part,stopwords)\n",
    "        train_corpus.append(doc)\n",
    "        \n",
    "    for ID in split['v_test']+ split['nv_test']:\n",
    "        doc = getPart(ID,part,stopwords)\n",
    "        test_corpus.append(doc)\n",
    "        \n",
    "    train_target = [1]*len(split['v_train']) + [0]*len(split['nv_train'])\n",
    "    test_target = [1]*len(split['v_test']) + [0]*len(split['nv_test'])\n",
    "    \n",
    "    \n",
    "    return train_corpus, test_corpus, train_target, test_target\n",
    "    \n",
    "    \n",
    "train_corpus, test_corpus, train_target, test_target = getCorpus('6','procedure','none')\n",
    "print(len(train_corpus),len(test_corpus),len(train_target),sum(train_target),len(test_target),sum(test_target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accordance</th>\n",
       "      <th>accordance with</th>\n",
       "      <th>accordance with rule</th>\n",
       "      <th>accordance with the</th>\n",
       "      <th>addresses</th>\n",
       "      <th>addresses by</th>\n",
       "      <th>addresses by mr</th>\n",
       "      <th>admissibility</th>\n",
       "      <th>admissibility and</th>\n",
       "      <th>admissibility and merits</th>\n",
       "      <th>...</th>\n",
       "      <th>would represent him rule</th>\n",
       "      <th>would submit</th>\n",
       "      <th>would submit his</th>\n",
       "      <th>would submit his observations</th>\n",
       "      <th>written</th>\n",
       "      <th>written observations</th>\n",
       "      <th>written procedure</th>\n",
       "      <th>written procedure article</th>\n",
       "      <th>written procedure article of</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accordance  accordance with  accordance with rule  accordance with the  \\\n",
       "0         0.0              0.0                   0.0                  0.0   \n",
       "1         0.0              0.0                   0.0                  0.0   \n",
       "2         0.0              0.0                   0.0                  0.0   \n",
       "3         0.0              0.0                   0.0                  0.0   \n",
       "4         0.0              0.0                   0.0                  0.0   \n",
       "\n",
       "   addresses  addresses by  addresses by mr  admissibility  admissibility and  \\\n",
       "0        0.0           0.0              0.0            0.0                0.0   \n",
       "1        0.0           0.0              0.0            1.0                1.0   \n",
       "2        0.0           0.0              0.0            0.0                0.0   \n",
       "3        0.0           0.0              0.0            0.5                0.0   \n",
       "4        0.0           0.0              0.0            0.5                0.0   \n",
       "\n",
       "   admissibility and merits  ...  would represent him rule  would submit  \\\n",
       "0                       0.0  ...                       0.0           0.0   \n",
       "1                       1.0  ...                       0.0           0.0   \n",
       "2                       0.0  ...                       0.0           0.0   \n",
       "3                       0.0  ...                       0.0           0.0   \n",
       "4                       0.0  ...                       0.0           0.0   \n",
       "\n",
       "   would submit his  would submit his observations  written  \\\n",
       "0               0.0                            0.0      0.0   \n",
       "1               0.0                            0.0      0.0   \n",
       "2               0.0                            0.0      0.0   \n",
       "3               0.0                            0.0      0.0   \n",
       "4               0.0                            0.0      0.0   \n",
       "\n",
       "   written observations  written procedure  written procedure article  \\\n",
       "0                   0.0                0.0                        0.0   \n",
       "1                   0.0                0.0                        0.0   \n",
       "2                   0.0                0.0                        0.0   \n",
       "3                   0.0                0.0                        0.0   \n",
       "4                   0.0                0.0                        0.0   \n",
       "\n",
       "   written procedure article of  target  \n",
       "0                           0.0       1  \n",
       "1                           0.0       1  \n",
       "2                           0.0       1  \n",
       "3                           0.0       1  \n",
       "4                           0.0       1  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram(article,part,stopwords):\n",
    "    \"\"\"\n",
    "    Create ngram features for article and hyper parameters\n",
    "    \"\"\"\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    max_features = 2000\n",
    "    ngram_range = (1,4)\n",
    "    \n",
    "    train_corpus, test_corpus, train_target, test_target = getCorpus(article,part,stopwords)\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features= max_features,\n",
    "                                 ngram_range=ngram_range)\n",
    "    #Train features\n",
    "    train = vectorizer.fit_transform(train_corpus,y=train_target).toarray()\n",
    "    train = scaler.fit_transform(train)\n",
    "    df_train = pd.DataFrame(data = train, columns=vectorizer.get_feature_names())\n",
    "    df_train['target'] = train_target\n",
    "    \n",
    "    #Test features\n",
    "    test = vectorizer.transform(test_corpus).toarray() #check what what y means\n",
    "    test = scaler.transform(test)\n",
    "    df_test = pd.DataFrame(data = test, columns=vectorizer.get_feature_names())\n",
    "    df_test['target'] = test_target\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = ngram('11','procedure','none')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 'ngram_2000_9_circumstances_english',\n",
    "file = test[6]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],ngram)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/test_features/{}.csv\"\n",
    "\n",
    "\n",
    "filename = \"{}_ngram_{}_{}_{}_{}\"\n",
    "train_filename = filename.format(\"train\",\"2000\",\"9\",\"circumstances\",\"english\")\n",
    "test_filename = filename.format(\"test\",\"2000\",\"9\",\"circumstances\",\"english\")\n",
    "\n",
    "df_train, df_test = ngram(\"9\",\"circumstances\",\"english\")\n",
    "\n",
    "df_train.to_csv(path.format(train_filename),index=False)\n",
    "df_test.to_csv(path.format(test_filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "9\n",
      "11\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/features2/{}.csv\"\n",
    "\n",
    "for article in [\"7\",\"9\",\"11\",\"18\"]:\n",
    "    for part in ['both',\"procedure\",\"facts\",\"circumstances\",\"relevant\"]:\n",
    "        for stopwords in ['none','english']:\n",
    "            \n",
    "            filename = \"{}_ngram_{}_{}_{}_{}\"\n",
    "            train_filename = filename.format(\"train\",\"2000\",article,part,stopwords)\n",
    "            test_filename = filename.format(\"test\",\"2000\",article,part,stopwords)\n",
    "            \n",
    "            df_train, df_test = ngram(article,part,stopwords)\n",
    "            \n",
    "            df_train.to_csv(path.format(train_filename),index=False)\n",
    "            df_test.to_csv(path.format(test_filename),index=False)\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/miniconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECHR\n",
      "LAW\n",
      "GLOVE\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "echr_100 = Word2Vec.load(path.format('echt2vec_100.txt'))\n",
    "echr_200 = Word2Vec.load(path.format('echt2vec_200.txt'))\n",
    "print(\"ECHR\")\n",
    "\n",
    "law_100 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.100d.txt'), binary=False)\n",
    "law_200 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.200d.txt'), binary=False)\n",
    "print(\"LAW\")\n",
    "word_100 = dict()\n",
    "f = open(path.format('glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_100[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "word_200 = dict()\n",
    "f = open(path.format('glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_200[word] = coefs\n",
    "f.close()\n",
    "print(\"GLOVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def averageVectors(corpus,embeddings):\n",
    "    \"\"\"\n",
    "    returns and average embedding feature set \n",
    "    \"\"\"\n",
    "    \n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    #stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "    for doc in corpus:\n",
    "        \n",
    "        series = pd.Series(doc.split(' ')).value_counts()\n",
    "        labels = series.index\n",
    "        counts = series.values\n",
    "        denom = sum(counts)\n",
    "        \n",
    "        temp = []\n",
    "        for word,count in zip(labels,counts): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word_vec = embeddings[word]*count # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                temp.append(word_vec) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = sum(temp)/denom \n",
    "\n",
    "        docs_vectors = docs_vectors.append(pd.Series(doc_vector), ignore_index = True) # append each document value to the final dataframe\n",
    "    \n",
    "        \n",
    "    return docs_vectors\n",
    "\n",
    "def averageEmbedding(article,part,stopwords,embeddings):\n",
    "    \"\"\"\n",
    "    Return the average embedding features sets\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    train_corpus, test_corpus, train_target, test_target = getCorpus(article,part,stopwords)\n",
    "\n",
    "    train = averageVectors(train_corpus,embeddings).fillna(0).values\n",
    "    train = scaler.fit_transform(train)\n",
    "    df_train = pd.DataFrame(data = train)\n",
    "    df_train['target'] = train_target\n",
    "    \n",
    "    #Test features\n",
    "    test = averageVectors(test_corpus,embeddings).fillna(0).values\n",
    "    test = scaler.transform(test)\n",
    "    df_test = pd.DataFrame(data = test)\n",
    "    df_test['target'] = test_target\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = averageEmbedding('2','both','none',word_100)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveWordEmbeddings(df_train,df_test, filename, article,part,stopwords):\n",
    "    train_filename = filename.format(\"train\",article,part,stopwords)\n",
    "    test_filename = filename.format(\"test\",article,part,stopwords)\n",
    "    \n",
    "    df_train.to_csv(path.format(train_filename),index=False)\n",
    "    df_test.to_csv(path.format(test_filename),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['law_100_2_circumstances_english',\n",
    " 'glove_200_3_both_none',\n",
    " 'glove_200_5_relevant_english',\n",
    " 'echr_100_6_both_english',\n",
    " 'glove_200_7_circumstances_none',\n",
    " 'echr_100_8_both_english',\n",
    " 'ngram_2000_9_circumstances_english',\n",
    " 'echr_200_10_both_none',\n",
    " 'glove_200_11_both_english',\n",
    " 'glove_100_13_both_english',\n",
    " 'echr_200_14_both_none',\n",
    " 'echr_200_18_both_none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 66\n",
      "490 202\n",
      "332 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 595\n",
      "64 9\n",
      "542 123\n",
      "256 59\n",
      "46 17\n",
      "202 149\n",
      "364 40\n",
      "26 3\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/test_features/{}.csv\"\n",
    "\n",
    "file = test[0]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],law_100)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "file = test[1]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],word_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "file = test[2]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],word_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "file = test[3]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],echr_100)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "file = test[4]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],word_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "file = test[5]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],echr_100)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "\n",
    "file = test[7]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],echr_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "file = test[8]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],word_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "file = test[9]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],word_100)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    " \n",
    "file = test[10]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],echr_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n",
    "\n",
    "\n",
    "file = test[11]\n",
    "filename = \"{}_\" + file\n",
    "split = file.split(\"_\")\n",
    "df_train, df_test = averageEmbedding(split[2],split[3],split[4],echr_200)\n",
    "print(len(df_train),len(df_test))\n",
    "saveWordEmbeddings(df_train, df_test,filename,split[2],split[3],split[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc4a355d31c44e4ae3c9ac5b4e19a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for article in tqdm_notebook([\"7\",\"9\",\"11\",\"18\"]):\n",
    "    for part in ['both',\"procedure\",\"facts\",\"circumstances\",\"relevant\"]:\n",
    "        for stopwords in ['none','english']:\n",
    "            \n",
    "            #glove 100\n",
    "            filename = \"{}_glove_100_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,word_100)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            #glove 200\n",
    "            filename = \"{}_glove_200_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,word_200)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            \n",
    "\n",
    "            #law 100\n",
    "            filename = \"{}_law_100_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,law_100)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            #law 200\n",
    "            filename = \"{}_law_200_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,law_200)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "\n",
    "            \n",
    "            #echr 100\n",
    "            filename = \"{}_echr_100_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,echr_100)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            #echr 200\n",
    "            filename = \"{}_echr_200_{}_{}_{}\"\n",
    "            df_train, df_test = averageEmbedding(article,part,stopwords,echr_200)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            \n",
    "            #doc 100\n",
    "            filename = \"{}_doc_100_{}_{}_{}\"\n",
    "            df_train, df_test = docEmbedding(article,part,stopwords,doc_100)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            #doc 200\n",
    "            filename = \"{}_doc_200_{}_{}_{}\"\n",
    "            df_train, df_test = docEmbedding(article,part,stopwords,doc_200)\n",
    "            saveWordEmbeddings(df_train, df_test,filename,article,part,stopwords)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoadModels\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "doc_100 = Doc2Vec.load(path.format('doc2vec_100')) \n",
    "doc_200 = Doc2Vec.load(path.format('doc2vec_200')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docVectors(corpus,model):\n",
    "    \"\"\"\n",
    "    returns and average embedding feature set \n",
    "    \"\"\"\n",
    "    \n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    for doc in corpus:\n",
    "       \n",
    "        doc_vector = pd.Series(model.infer_vector(doc,steps = 20))\n",
    "        \n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "    return docs_vectors\n",
    "    \n",
    "def docEmbedding(article,part,stopwords,model):\n",
    "    \"\"\"\n",
    "    Return the average embedding features sets\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    train_corpus, test_corpus, train_target, test_target = getCorpus(article,part,stopwords)\n",
    "\n",
    "    train = docVectors(train_corpus,model).fillna(0).values\n",
    "    train = scaler.fit_transform(train)\n",
    "    df_train = pd.DataFrame(data = train)\n",
    "    df_train['target'] = train_target\n",
    "    \n",
    "    #Test features\n",
    "    test = docVectors(test_corpus,model).fillna(0).values\n",
    "    test = scaler.transform(test)\n",
    "    df_test = pd.DataFrame(data = test)\n",
    "    df_test['target'] = test_target\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.701853</td>\n",
       "      <td>0.288264</td>\n",
       "      <td>0.911129</td>\n",
       "      <td>0.817519</td>\n",
       "      <td>0.666821</td>\n",
       "      <td>0.617580</td>\n",
       "      <td>0.757744</td>\n",
       "      <td>0.362429</td>\n",
       "      <td>0.336497</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201242</td>\n",
       "      <td>0.292513</td>\n",
       "      <td>0.140923</td>\n",
       "      <td>0.773359</td>\n",
       "      <td>0.810436</td>\n",
       "      <td>0.500381</td>\n",
       "      <td>0.120092</td>\n",
       "      <td>0.199455</td>\n",
       "      <td>0.194301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.871552</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.487415</td>\n",
       "      <td>0.991884</td>\n",
       "      <td>0.714283</td>\n",
       "      <td>0.772244</td>\n",
       "      <td>0.857719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249762</td>\n",
       "      <td>0.770412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.108396</td>\n",
       "      <td>0.169406</td>\n",
       "      <td>0.764955</td>\n",
       "      <td>0.768360</td>\n",
       "      <td>0.028876</td>\n",
       "      <td>0.228581</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>0.276292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.741251</td>\n",
       "      <td>0.194626</td>\n",
       "      <td>0.733237</td>\n",
       "      <td>0.806997</td>\n",
       "      <td>0.723123</td>\n",
       "      <td>0.692224</td>\n",
       "      <td>0.858634</td>\n",
       "      <td>0.421790</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>0.484320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185298</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.167108</td>\n",
       "      <td>0.686322</td>\n",
       "      <td>0.697989</td>\n",
       "      <td>0.579541</td>\n",
       "      <td>0.176833</td>\n",
       "      <td>0.212538</td>\n",
       "      <td>0.121831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.812235</td>\n",
       "      <td>0.215559</td>\n",
       "      <td>0.929595</td>\n",
       "      <td>0.848726</td>\n",
       "      <td>0.762768</td>\n",
       "      <td>0.849760</td>\n",
       "      <td>0.675506</td>\n",
       "      <td>0.600225</td>\n",
       "      <td>0.284654</td>\n",
       "      <td>0.984758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198924</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.409667</td>\n",
       "      <td>0.799542</td>\n",
       "      <td>0.714503</td>\n",
       "      <td>0.518365</td>\n",
       "      <td>0.208994</td>\n",
       "      <td>0.087479</td>\n",
       "      <td>0.494510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809568</td>\n",
       "      <td>0.309279</td>\n",
       "      <td>0.455995</td>\n",
       "      <td>0.746146</td>\n",
       "      <td>0.633901</td>\n",
       "      <td>0.597871</td>\n",
       "      <td>0.706190</td>\n",
       "      <td>0.376005</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.642230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257492</td>\n",
       "      <td>0.347788</td>\n",
       "      <td>0.194931</td>\n",
       "      <td>0.699961</td>\n",
       "      <td>0.822112</td>\n",
       "      <td>0.651407</td>\n",
       "      <td>0.180120</td>\n",
       "      <td>0.299722</td>\n",
       "      <td>0.790947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.701853  0.288264  0.911129  0.817519  0.666821  0.617580  0.757744   \n",
       "1  0.871552  0.038642  0.487415  0.991884  0.714283  0.772244  0.857719   \n",
       "2  0.741251  0.194626  0.733237  0.806997  0.723123  0.692224  0.858634   \n",
       "3  0.812235  0.215559  0.929595  0.848726  0.762768  0.849760  0.675506   \n",
       "4  0.809568  0.309279  0.455995  0.746146  0.633901  0.597871  0.706190   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0  0.362429  0.336497  0.626623  ...  0.201242  0.292513  0.140923  0.773359   \n",
       "1  1.000000  0.249762  0.770412  ...  0.216814  0.108396  0.169406  0.764955   \n",
       "2  0.421790  0.157690  0.484320  ...  0.185298  0.182500  0.167108  0.686322   \n",
       "3  0.600225  0.284654  0.984758  ...  0.198924  0.232102  0.409667  0.799542   \n",
       "4  0.376005  0.202541  0.642230  ...  0.257492  0.347788  0.194931  0.699961   \n",
       "\n",
       "         95        96        97        98        99  target  \n",
       "0  0.810436  0.500381  0.120092  0.199455  0.194301       1  \n",
       "1  0.768360  0.028876  0.228581  0.077343  0.276292       1  \n",
       "2  0.697989  0.579541  0.176833  0.212538  0.121831       1  \n",
       "3  0.714503  0.518365  0.208994  0.087479  0.494510       1  \n",
       "4  0.822112  0.651407  0.180120  0.299722  0.790947       1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = docEmbedding('2','procedure','english',doc_100)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/features2/{}.pickle\"\n",
    "with open(path.format('features'), 'wb') as f:\n",
    "\n",
    "    for article in [\"7\",\"9\",\"11\",\"18\"]:\n",
    "        for part in ['both',\"procedure\",\"facts\",\"circumstances\",\"relevant\"]:\n",
    "            for stopwords in ['none','english']:\n",
    "            \n",
    "                filename = \"{}_{}_{}_{}_{}\"\n",
    "                \n",
    "                features.append(filename.format(\"ngram\",\"2000\",article,part,stopwords))\n",
    "                \n",
    "                features.append(filename.format(\"glove\",\"100\",article,part,stopwords))\n",
    "                features.append(filename.format(\"glove\",\"200\",article,part,stopwords))\n",
    "\n",
    "                features.append(filename.format(\"law\",\"100\",article,part,stopwords))\n",
    "                features.append(filename.format(\"law\",\"200\",article,part,stopwords))\n",
    "                \n",
    "                features.append(filename.format(\"echr\",\"100\",article,part,stopwords))\n",
    "                features.append(filename.format(\"echr\",\"200\",article,part,stopwords))\n",
    "                \n",
    "                features.append(filename.format(\"doc\",\"100\",article,part,stopwords))\n",
    "                features.append(filename.format(\"doc\",\"200\",article,part,stopwords))\n",
    "                \n",
    "                \n",
    "    pickle.dump(features,f)\n",
    "    \n",
    "    f.close()\n",
    "print(len(features)  )        \n",
    "                \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageVectorsOLD(corpus,embeddings):\n",
    "    \"\"\"\n",
    "    returns and average embedding feature set \n",
    "    \"\"\"\n",
    "    \n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    #stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "    for doc in corpus:\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "\n",
    "        \n",
    "    return docs_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
