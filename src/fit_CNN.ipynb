{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('train_test_split'), 'r') as readfile:\n",
    "    train_test_split = json.load(readfile)\n",
    "    readfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open cases\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_none'), 'r') as readfile:\n",
    "    text_none = json.load(readfile)\n",
    "    readfile.close()\n",
    "    \n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_english'), 'r') as readfile:\n",
    "    text_english = json.load(readfile)\n",
    "    readfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECHR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAW\n",
      "GLOVE\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "echr_100 = Word2Vec.load(path.format('echt2vec_100.txt'))\n",
    "echr_200 = Word2Vec.load(path.format('echt2vec_200.txt'))\n",
    "print(\"ECHR\")\n",
    "\n",
    "law_100 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.100d.txt'), binary=False)\n",
    "law_200 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.200d.txt'), binary=False)\n",
    "print(\"LAW\")\n",
    "word_100 = dict()\n",
    "f = open(path.format('glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_100[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "word_200 = dict()\n",
    "f = open(path.format('glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_200[word] = coefs\n",
    "f.close()\n",
    "print(\"GLOVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 447 1008 504 447 391\n"
     ]
    }
   ],
   "source": [
    "# Fucntions\n",
    "def getPart(ID,part,stopwords):\n",
    "    \"\"\"\n",
    "    Returns clean case part\n",
    "    \"\"\"\n",
    "    \n",
    "    if stopwords == 'none':\n",
    "        doc = text_none[ID][part]\n",
    "    elif stopwords == 'english':\n",
    "        doc = text_english[ID][part]\n",
    "    return doc\n",
    "\n",
    "\n",
    "def getCorpus(article,part,stopwords):\n",
    "    \"\"\"\n",
    "    returns the train test corpus along with target\n",
    "    \"\"\"\n",
    "    split = train_test_split[article]\n",
    "    \n",
    "    train_corpus = []\n",
    "    test_corpus = []\n",
    "    \n",
    "    for ID in split['v_train'] + split['nv_train']:\n",
    "        doc = getPart(ID,part,stopwords)\n",
    "        train_corpus.append(doc)\n",
    "        \n",
    "    for ID in split['v_test']+ split['nv_test']:\n",
    "        doc = getPart(ID,part,stopwords)\n",
    "        test_corpus.append(doc)\n",
    "        \n",
    "    train_target = [1]*len(split['v_train']) + [0]*len(split['nv_train'])\n",
    "    test_target = [1]*len(split['v_test']) + [0]*len(split['nv_test'])\n",
    "    \n",
    "    \n",
    "    return train_corpus, test_corpus, train_target, test_target\n",
    "    \n",
    "    \n",
    "train_corpus, test_corpus, train_target, test_target = getCorpus('6','procedure','none')\n",
    "print(len(train_corpus),len(test_corpus),len(train_target),sum(train_target),len(test_target),sum(test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitCNN(article,part,stopwords,embedding,dimension,batch_size,num_epochs,num_filters,kernel_size):\n",
    "    \n",
    "    #training params\n",
    "    batch_size = batch_size\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "    #model parameters\n",
    "    num_filters = num_filters\n",
    "    kernel_size = kernel_size\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    train_corpus, test_corpus, train_target, test_target = getCorpus('6','procedure','none')\n",
    "    docs,labels = train_corpus,train_target\n",
    "    \n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    # integer encode the documents\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    doc_length = [len(x) for x in encoded_docs]\n",
    "    #print(encoded_docs)\n",
    "    # pad documents to a max length of 4 words\n",
    "    max_length = max(doc_length)\n",
    "    print(max_length)\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    #print(padded_docs)\n",
    "\n",
    "    embeddings_index = embedding\n",
    "    \n",
    "\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size, dimension))\n",
    "    for word, i in t.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(len(embedding_matrix))\n",
    "    \n",
    "    \n",
    "    X = np.array(padded_docs)\n",
    "    Y = np.array(labels)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=90)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "      # create model\n",
    "        embedding_layer = Embedding(vocab_size, \n",
    "                                    dimension, \n",
    "                                    weights=[embedding_matrix], \n",
    "                                    input_length=max_length, \n",
    "                                    trainable=False)\n",
    "          \n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Conv1D(num_filters, 3, activation='relu', padding='same'))\n",
    "        model.add(Conv1D(num_filters, 4, activation='relu', padding='same'))\n",
    "        model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        \"\"\"model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "        model.add(GlobalMaxPooling1D())\"\"\"\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=ada, metrics=['accuracy'])\n",
    "        \n",
    "        model.fit(X[train], Y[train], epochs=10, verbose=0,batch_size=128)\n",
    "        \n",
    "            # evaluate the model\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "echr\t100\tArticle 6\tboth\tenglish\t0.755863\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 70.79%\n",
      "acc: 65.35%\n",
      "acc: 70.30%\n",
      "acc: 70.79%\n",
      "acc: 68.00%\n",
      "69.05% (+/- 2.12%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 100\n",
    "kernel_size = 3\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 71.29%\n",
      "acc: 70.79%\n",
      "acc: 70.30%\n",
      "acc: 73.27%\n",
      "acc: 71.50%\n",
      "71.43% (+/- 1.01%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =5\n",
    "num_filters = 100\n",
    "kernel_size = 5\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 68.81%\n",
      "acc: 70.30%\n",
      "acc: 69.80%\n",
      "acc: 71.29%\n",
      "acc: 74.50%\n",
      "70.94% (+/- 1.95%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =15\n",
    "num_filters = 100\n",
    "kernel_size = 5\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 69.80%\n",
      "acc: 66.83%\n",
      "acc: 69.31%\n",
      "acc: 74.75%\n",
      "acc: 72.50%\n",
      "70.64% (+/- 2.73%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 50\n",
    "kernel_size = 5\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 67.82%\n",
      "acc: 67.33%\n",
      "acc: 70.79%\n",
      "acc: 72.28%\n",
      "acc: 73.50%\n",
      "70.34% (+/- 2.42%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 150\n",
    "kernel_size = 5\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 71.78%\n",
      "acc: 68.32%\n",
      "acc: 71.78%\n",
      "acc: 78.71%\n",
      "acc: 78.50%\n",
      "73.82% (+/- 4.11%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 100\n",
    "kernel_size = 10\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 70.30%\n",
      "acc: 65.35%\n",
      "acc: 71.78%\n",
      "acc: 73.76%\n",
      "acc: 72.50%\n",
      "70.74% (+/- 2.92%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 100\n",
    "kernel_size = 20\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 66.83%\n",
      "acc: 63.86%\n",
      "acc: 72.77%\n",
      "acc: 68.81%\n",
      "acc: 69.50%\n",
      "68.36% (+/- 2.95%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 100\n",
    "kernel_size = 30\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n",
      "7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 71.78%\n",
      "acc: 65.35%\n",
      "acc: 67.82%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e1e0de3f2d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfitCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mechr_200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-5440ac41fa8c>\u001b[0m in \u001b[0;36mfitCNN\u001b[0;34m(article, part, stopwords, embedding, dimension, batch_size, num_epochs, num_filters, kernel_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/tensor/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/virtualenv/tensor/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/tensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/tensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs =10\n",
    "num_filters = 100\n",
    "kernel_size = 50\n",
    "\n",
    "fitCNN('6','both','english',echr_200,200,batch_size,num_epochs,num_filters,kernel_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
