{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9703\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUDOC-ECHR-1982-001-57417</td>\n",
       "      <td>1982-03-26</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUDOC-ECHR-1982-001-57580</td>\n",
       "      <td>1982-09-23</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUDOC-ECHR-1983-001-57554</td>\n",
       "      <td>1983-04-25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUDOC-ECHR-1983-001-57591</td>\n",
       "      <td>1983-11-23</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUDOC-ECHR-1984-001-57465</td>\n",
       "      <td>1984-10-26</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id        date  2  3  5  6  7  8  9  10  11  13  14  \\\n",
       "0  HUDOC-ECHR-1982-001-57417  1982-03-26 -1 -1 -1  0 -1 -1 -1  -1  -1  -1  -1   \n",
       "1  HUDOC-ECHR-1982-001-57580  1982-09-23 -1 -1 -1  1 -1 -1 -1  -1  -1  -1   0   \n",
       "2  HUDOC-ECHR-1983-001-57554  1983-04-25 -1 -1 -1  1 -1 -1 -1  -1  -1  -1  -1   \n",
       "3  HUDOC-ECHR-1983-001-57591  1983-11-23 -1 -1 -1 -1 -1 -1 -1  -1  -1  -1   0   \n",
       "4  HUDOC-ECHR-1984-001-57465  1984-10-26 -1 -1 -1  1 -1 -1 -1  -1  -1  -1  -1   \n",
       "\n",
       "   18  \n",
       "0  -1  \n",
       "1  -1  \n",
       "2  -1  \n",
       "3  -1  \n",
       "4  -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv('/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/target.csv')\n",
    "print(len(target))\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9703\n",
      "HUDOC-ECHR-1982-001-57417\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Open cases\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/clean/{}.json\"\n",
    "\n",
    "with open(path.format('text_none'), 'r') as readfile:\n",
    "    text = json.load(readfile)\n",
    "    readfile.close()\n",
    "    \n",
    "print(len(text.keys()))\n",
    "print(list(text.keys())[0])\n",
    "print(len(text['HUDOC-ECHR-2012-001-110881']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding = 'glove.6B.100d.txt'\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embed_path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open(embed_path.format(embedding), encoding='utf-8',errors='ignore')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECHR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conorosully/virtualenv/tensor/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAW\n",
      "GLOVE\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "echr_100 = Word2Vec.load(path.format('echt2vec_100.txt'))\n",
    "echr_200 = Word2Vec.load(path.format('echt2vec_200.txt'))\n",
    "print(\"ECHR\")\n",
    "\n",
    "law_100 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.100d.txt'), binary=False)\n",
    "law_200 = gensim.models.KeyedVectors.load_word2vec_format(path.format('Law2Vec.200d.txt'), binary=False)\n",
    "print(\"LAW\")\n",
    "word_100 = dict()\n",
    "f = open(path.format('glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_100[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "word_200 = dict()\n",
    "f = open(path.format('glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_200[word] = coefs\n",
    "f.close()\n",
    "print(\"GLOVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSet(article,part,stop_words = None):\n",
    "    \"\"\"\n",
    "    returns the feature set corrisponding to article and part \n",
    "    \"\"\"\n",
    "    df = target[['id',article]].sample(frac=1)\n",
    "    \n",
    "    #Get balanced dataset\n",
    "    nvLen = len(df[df[article] == 0]) \n",
    "    vLen = len(df[df[article] == 1])\n",
    "    minLen = min(nvLen,vLen)\n",
    "\n",
    "    nvID = df[df[article] == 0][0:minLen]['id']\n",
    "    vID = df[df[article] == 1][0:minLen]['id']\n",
    "    \n",
    "    nvCorpus = []\n",
    "    vCorpus = []\n",
    "    for ID in nvID:\n",
    "        nvCorpus.append(text[ID][part])\n",
    "\n",
    "    for ID in vID:\n",
    "        vCorpus.append(text[ID][part])\n",
    "        \n",
    "    corpus = nvCorpus + vCorpus\n",
    "    targets = [0]*minLen + [1]*minLen\n",
    "    print(\"corpus\",len(corpus))\n",
    "    \n",
    "    return corpus, array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitCNN(article,part,max_length,embedding,dimension):\n",
    "    \n",
    "    #training params\n",
    "    batch_size = 256 \n",
    "    num_epochs = 8 \n",
    "\n",
    "    #model parameters\n",
    "    num_filters = 64 \n",
    "    embed_dim = 300 \n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    \n",
    "    article = article\n",
    "    part = part\n",
    "    docs,labels = featureSet(article,part,[])\n",
    "\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    # integer encode the documents\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    #print(encoded_docs)\n",
    "    # pad documents to a max length of 4 words\n",
    "    max_length = max_length\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    #print(padded_docs)\n",
    "\n",
    "    \"\"\"embed_path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "    try:\n",
    "        embeddings_index = Word2Vec.load(embed_path.format(embedding))\n",
    "    except:\n",
    "        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(embed_path.format(embedding), binary=False)\n",
    "    embeddings_index.init_sims(replace=True)\"\"\"\n",
    "    embeddings_index = embedding\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size, dimension))\n",
    "    for word, i in tqdm(t.word_index.items()):\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    \n",
    "    X = padded_docs\n",
    "    Y = labels\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "      # create model\n",
    "        embedding_layer = Embedding(vocab_size, \n",
    "                                    dimension, \n",
    "                                    weights=[embedding_matrix], \n",
    "                                    input_length=max_length, \n",
    "                                    trainable=False)\n",
    "        \n",
    "        \"\"\"embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Dense(1, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "        adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        \n",
    "        model.fit(X[train], Y[train], epochs=10, verbose=0,batch_size=128)\n",
    "        \n",
    "            # evaluate the model\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8118/8118 [00:00<00:00, 222203.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 76.34%\n",
      "acc: 71.43%\n",
      "acc: 69.64%\n",
      "acc: 67.41%\n",
      "acc: 65.18%\n",
      "70.00% (+/- 3.80%)\n"
     ]
    }
   ],
   "source": [
    "fitCNN('6','procedure',2000,law_100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(article,part,max_length,embedding,dimension):\n",
    "    article = article\n",
    "    part = part\n",
    "    docs,labels = featureSet(article,part,[])\n",
    "\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    # integer encode the documents\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    #print(encoded_docs)\n",
    "    # pad documents to a max length of 4 words\n",
    "    max_length = max_length\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    #print(padded_docs)\n",
    "\n",
    "    embed_path = \"/Users/conorosully/Documents/Legal-Case-Prediction/data/embeddings/{}\"\n",
    "    try:\n",
    "        embeddings_index = Word2Vec.load(embed_path.format(embedding))\n",
    "    except:\n",
    "        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(embed_path.format(embedding), binary=False)\n",
    "    embeddings_index.init_sims(replace=True)\n",
    "\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size, dimension))\n",
    "    for word, i in tqdm(t.word_index.items()):\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    \n",
    "    X = padded_docs\n",
    "    Y = labels\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=90)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "      # create model\n",
    "        model = Sequential()\n",
    "        e = Embedding(vocab_size, dimension, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "        model.add(e)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])    # Fit the model\n",
    "\n",
    "        model.fit(X[train], Y[train], epochs=50, verbose=0,batch_size=20)\n",
    "            # evaluate the model\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
